<!DOCTYPE html>
<html>
<head>
    <title>Machine Learning Terms</title>
    <link rel="stylesheet" href="/css/styles.css"/>
</head>

<body>
    <nav class="navbar">    <!--See .navbar in styles.css-->
        <ul>
          <li><a href="/">Home</a></li>
          <li><a href="#">Our team</a></li>
          <li><a href="#">Projects</a></li>
          <li><a href="#">Contact</a></li>
          <li>
            <form>
                <input type="search" name="q" placeholder="Search site" />
                <input type="submit" value="Go!" />
            </form>
           </li>
        </ul>
    </nav>
    <aside class="sidebar">   <!-- Sidebar -->
        <a href="#comp">AI, ML, DL</a>
        <a href="#ActivationFunction">Activation Function</a>
        <a href="#conda">Conda</a>
        <a href="#opt">Optimizer, stochastic gradient descent (SGD)</a>
        <a href="#overfitting">Overfitting</a>
        <a href="#bias">Bias/Sampling Bias</a>
        <a href="#cntk">CNTK</a>
        <a href="#keras">Keras</a>
        <a href="#layer">Layer</a>
        <a href="#learningtypes">Learning Types</a>
        <a href="#lossf">Loss function</a>
        <a href="#neuralnetwork">Neural Network</a>
        <a href="#tensor">Tensor</a>
        <a href="#tensorflow">Tensorflow</a>
        <a href="#underfitting">Underfitting</a>
        <a href="#variance">Variance</a>
    </aside>

    <div style="margin-left:200px;">
        <h3 id="comp">Artificial Intelligence, Machine Learning, Deep Learning</h3>
        <table>
            <tr>
                <th></th>
                <th>AL</th>
                <th>ML</th>
                <th>Deep Learning</th>
            </tr>
            <tr>
                <td>Originated</td>
                <td>1950</td>
                <td>1960</td>
                <td>1970</td>
            </tr>
            <tr>
                <td>What</td>
                <td>Simulated Intelligence in Machines</td>
                <td>Machine making decisions without being programmed</td>
                <td>Using Neural networks to solve complex problems</td>
            </tr>
            <tr>
                <td>Objective</td>
                <td>Building machines which can think like humans</td>
                <td>Algo which can learn thru data</td>
                <td>Neural n/w to identify patterns</td>
            </tr>
        </table>

        <h3 id="ActivationFunction">Activation Function</h3>
        <dl>Without an activation function(eg: relu, softmax), [Dense layer](#layer) would always perform linear operations(a dot product, an addition) on input tensors.</dl>
        <dl>Adding activation function to a layer introduces non-linearity into the model, allowing it to learn more complex relationships between the input and output data</dl>
        <dl>Non-linear activation functions such as ReLU, Sigmoid, and Tanh can help the model to better fit the training data and make more accurate predictions on new data.</dl>

        <h3 id="conda">Conda</h3>
        <dl><a href="https://docs.conda.io/projects/miniconda/en/latest/">Miniconda</a> is the recommended approach for installing TensorFlow with GPU support</dl>
        <dl> It creates a separate environment to avoid changing any installed software in your system. This is also the easiest way to install the required software especially for the GPU setup.</dl>

        <h3 id="opt">Optimizer, stochastic gradient descent (SGD)</h3>
        <dl>Tells how parameters should be tuned to make model produce expected output. Presently ML model is showing [loss function](#lf).</dl>
        <dl>Goal of gradient descent is to identify the model parameters that provide the maximum accuracy.</dl>
        <dl>Gradient-descent process must be based on a single scalar loss value; so, for multiloss networks, all losses are combined (via averaging) into a single scalar quantity.</dl>

        <h3 id="overfitting">Overfitting</h3>
        <dl>Means that the model performs well on the training data, but it does not generalize well(ie produces good results on real world/unseen data), because there is too of much uneccessary data(noise) in traning data.</dl>
        <dl><strong>Regularization:</strong> Constraining a model to make it simpler and reduce the risk of overfitting.</dl>

        <h3 id="bias">Bias/Sampling Bias</h3>
        <dl>We should use a training data set that is representative of the cases we want model to predict.</dl>
        <dl>if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.</dl>

        <h3 id="cntk">CNTK</h3>
        <dl>This is Microsoft Cognitive Toolkit (CNTK) backend, plugged with keras.</dl>

        <h3 id="keras">Keras</h3>
        <dl>Library(in Python) which provides functions/APIs to build deep-learning models. Different backends can be plugged with keras</dl>
        <code><pre>
            Keras
            Tensorflow / Theano / CNTK
             CUDA           BLAS,Eigen
             GPU            CPU
        </pre></code>

        <h3 id="layer">Layer</h3>
        <dl>Neural network is created by cascading multiple layers.</dl>

        <h3 id="learningtypes">Learning Types</h3>
        <h4>A. Supervised Learning</h4>
        <dl>The training data feed to the algorithm includes the desired solutions(called labels).</dl>
        <dl>Supervised learning algorithms</dl>
            <dd>k-Nearest Neighbors, <br>
                Linear Regression, <br>Logistic Regression, <br>
                Support Vector Machines (SVMs), <br>Decision Trees & Random Forests, <br>
                Neural networks</dd>
        <dl><strong>Types of Supervised learning</strong></dl>
        <dt><u>1.Classification (give yes/no)</u></dt>
        <dd>a. Spam Filterning: Algo is trained with many example emails along with their class (spam or ham), 
            and it must learn how to classify new emails. Each email has a label.</dd>

        <dt><u>2. Regression: (give % or numeric value)</u></dt>
        <dd>Predictors(Predict something):<br>
            Predict whether based on inputs, Predict price of car provided with some inputs((mileage, age, brand, etc.)</dd>
        
        <dt>3. Logistic Regression (yes/no with %)</dt>
        <dd>Mix of classfication & Regression. Example: 20% of chances being a spam.</dd>

        <h4>B. Unsupervised Learning</h4>
        <dl>Dataset does not have labels. ML model tries to learn without teacher.</dl>
        <dl><strong>Types of unsupervised learning</strong></dl>
        <dt><u>1. Clustering</u></dt>
        <dd>Algorithms used: (k-Means, Hierarchical Cluster Analysis (HCA), Expectation Maximization)</dd>
        <dt><u>2. Visualization and dimensionality reduction</u></dt>
        <dd>
            With unlabelled data this algorithm provides ouput which can be plotted on 2-D, 3-D plane.<br>
            Algorithms used: <br>
            Principal Component Analysis (PCA), Kernel PCA, Locally-Linear Embedding (LLE), t-distributed Stochastic Neighbor Embedding (t-SNE)
        </dd>
        <dt><u>3. Association rule learning</u></dt>
        <dd>
            This provides output as relations between attributes.<br>
            Algorithms used: Apriori, Eclat<br>
            Examples:<br>
            1. Supermarket data analysis: suppose you own a supermarket. Sales logs 
            may reveal that people who purchase sauce, potato chips also buy bread. 
            Thus, you may want to place these items close to each other
        </dd>

        <h4>C. Semisupervised Learning</h4>
        <dt>lot of unlabeled data and a little bit of labeled data<br>
            Algorithms: Deep belief networks (DBNs),<br>
            Examples: 1. FB Photos: When we load photos, we provide labels to few and leave others.
             AI identifies photos.
        </dt>

        <h4>D. Reinforcement learning</h4>
        <dt>Agent(AI Program) can observe the environment, select and perform actions, and get rewards in return
        </dt>

        <h3 id="lossf">Loss Function</h3>
        <dl>Function that compares expected and actual values. Measures how well the neural network models the training data. 
            Loss function should be minimum.</dl>
        <code><pre>
            Loss function = (Actual O/P) - (Expected output)
        </pre></code>

        <h3 id="neuralnetwork">Neural Network</h3>
        <dl></dl>

        <h3 id="tensor">Tensor = n-D Matrix</h3>
        <dl>This is matrix(as in maths). 
            Multi-dimensional numpy arrays used to store numbers during computation.</dl>

        <h4>Types of Tensors</h4>
        <table>
            <tr>
                <th>Dimension/Rank/Axis/Ndim</th>
                <th>Name</th>
                <th>Representation</th>
                <th>Examples</th>
                <th>Shape(Rows,cols)</th>
                <th>Processed By (Keras)</th>
            </tr>
            <tr>
                <td>0</td>
                <td>Scalar</td>
                <td>[0]</td>
                <td></td>
                <td>(0)</td>
                <td></td>
            </tr>
            <tr>
                <td>1</td>
                <td>vector</td>
                <td>[1,2,3,4]</td>
                <td></td>
                <td>(4)</td>
                <td></td>
            </tr>
            <tr>
                <td>2</td>
                <td>Matrix / 2D Tensor</td>
                <td>{{1,2,3},{4,5,6}}</td>
                <td>samples</td>
                <td>(2,3)</td>
                <td>Dense Layer</td>
            </tr>
            <tr>
                <td>3</td>
                <td>3D Tensor</td>
                <td>{{{1,2,3},{4,5,6}},{{1,2,3},{4,5,6}}}</td>
                <td>Timestamped data</td>
                <td>(2,2,3)</td>
                <td>Recurrent layers(eg: LSTM layer)</td>
            </tr>
            <tr>
                <td>4</td>
                <td>4D Tensor</td>
                <td>3D tensors packed together</td>
                <td></td>
                <td>2D convolution layers (Conv2D)</td>
                <td></td>
            </tr>
        </table>
        <code><pre>
            /////////// 2-D Tensor //////////////
            Shape: (2,3)
            Dimension/Rank/Axis/Ndim: 2
                    | 1,2,3 |
                    | 4,5,6 |
            
            /////////// 3-D Tensor example. Packing 2-D matrices //////////////
            Shape: (3,3,5)
            Dimension/Rank/Axis/Ndim: 3
            
            >> x = np.array([[
                                  [5, 78, 2, 34, 0],
                                  [6, 79, 3, 35, 1],
                                  [7, 80, 4, 36, 2]
                             ],
                             [
                                  [5, 78, 2, 34, 0],
                                  [6, 79, 3, 35, 1],
                                  [7, 80, 4, 36, 2]
                             ],
                             [
                                  [5, 78, 2, 34, 0],
                                  [6, 79, 3, 35, 1],
                                  [7, 80, 4, 36, 2]
                            ]])
        </pre></code>

        <h4>Tensor Terms</h4>
        <dl>Data types(dtype)</dl>
        <dd>Type of the data contained in the tensor; for instance, a tensor’s type could be float32, uint8, float64, and so on.</dd>
        <dd>String tensors don’t exist in Numpy (or in most other libraries), because tensors are preallocated contiguous memory segments, and strings, being variable length.</dd>

        <dl>Rank/Axis/Dimension/ndim</dl>
        <dd>Dimension of matrix. For instance, a 3D tensor has three axes, and a matrix has two axes. This is also called the tensor’s ndim in Python libraries such as Numpy.</dd>
        
        <dl>Shape</dl>
        <dd>Tells how many size tensor has along each axis.</dd>
        <dd>For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5).</dd>

        <h3 id="tensorflow">Tensorflow</h3>
        <dl>This is ML Open source library(EXPOSING APIs) for numerical computation and large-scale ML supports CPUs & GPUs.</dl>
        <dl>Python Front-end APIs & backend written in c++ for high performance.</dl>
        <code><pre>
            //Install conda https://docs.conda.io/projects/miniconda/en/latest/
            C:\Users\amitk\source\repos\Python> mkdir venv_ml1
            C:\Users\amitk\source\repos\Python> cd venv_ml1
            C:\Users\amitk\source\repos\Python\venv_ml1>"c:\Users\amitk\miniconda3\Scripts\activate" venv_ml1
            //Env is created here: C:\Users\amitk\miniconda3\envs
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>"c:\Users\amitk\miniconda3\condabin\deactivate.bat"  //deactivate
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>pip install tensorflow
            Downloading tensorflow-2.6.2-cp36-cp36m-win_amd64.whl (423.3 MB)
        </pre></code>

        <h3 id="underfitting">Underfitting</h3>
        <dl>Does not produces good results on traning data.</dl>

        <h3 id="variance">Variance</h3>
        <dl>Variance is the tendency to learn random things unrelated to the real signal</dl>

    </div>
</body>
</html>
