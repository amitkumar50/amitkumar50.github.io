<!DOCTYPE html>
<html>
<head>
    <title>Machine Learning Terms</title>
    <link rel="stylesheet" href="/css/styles.css"/>
</head>

<body>
    <nav class="navbar">    <!--See .navbar in styles.css-->
        <ul>
          <li><a href="/">Home</a></li>
          <li><a href="#">Our team</a></li>
          <li><a href="#">Projects</a></li>
          <li><a href="#">Contact</a></li>
          <li>
            <form>
                <input type="search" name="q" placeholder="Search site" />
                <input type="submit" value="Go!" />
            </form>
           </li>
        </ul>
    </nav>
    <aside class="sidebar">   <!-- Sidebar -->
        <a href="#comp">AI, ML, DL</a>
        <a href="#ActivationFunction">Activation Function</a>
        <a href="#conda">Conda</a>
        <a href="#opt">Optimizer, stochastic gradient descent (SGD)</a>
        <a href="#overfitting">Overfitting</a>
        <a href="#bias">Bias/Sampling Bias</a>
        <a href="#cntk">CNTK</a>
        <a href="#keras">Keras</a>
        <a href="#layer">Layer</a>
        <a href="#learningtypes">Learning Types</a>
        <a href="#lossf">Loss function</a>
        <a href="#neuron">Neuron</a>
        <a href="#neuralnetwork">Neural Network</a>
        <a href="#tensor">Tensor</a>
        <a href="#tensorflow">Tensorflow</a>
        <a href="#underfitting">Underfitting</a>
        <a href="#variance">Variance</a>
    </aside>

    <div style="margin-left:200px;">
        <h3 id="comp">Artificial Intelligence, Machine Learning, Deep Learning</h3>
        <table>
            <tr>
                <th></th>
                <th>AL</th>
                <th>ML</th>
                <th>Deep Learning</th>
            </tr>
            <tr>
                <td>Originated</td>
                <td>1950</td>
                <td>1960</td>
                <td>1970</td>
            </tr>
            <tr>
                <td>What</td>
                <td>Simulated Intelligence in Machines</td>
                <td>Machine making decisions without being programmed</td>
                <td>Using Neural networks to solve complex problems</td>
            </tr>
            <tr>
                <td>Objective</td>
                <td>Building machines which can think like humans</td>
                <td>Algo which can learn thru data</td>
                <td>Neural n/w to identify patterns</td>
            </tr>
        </table>

        <h3 id="ActivationFunction">Activation Function</h3>
        <dl>Without an activation function(eg: relu, softmax), 
            <a href="#layer">Dense layer</a> would always perform linear operations
            (a dot product, an addition) on input tensors.</dl>
        <dl>Adding activation function to a layer introduces non-linearity into the model, 
            allowing it to learn more complex relationships between the input and output data</dl>
        <dl>Non-linear activation functions such as ReLU, Sigmoid, and Tanh can help the 
            model to better fit the training data and make more accurate predictions on new data.</dl>

        <h3 id="conda">Conda</h3>
        <dl><a href="https://docs.conda.io/projects/miniconda/en/latest/">Miniconda</a> is the recommended approach for installing TensorFlow with GPU support</dl>
        <dl> It creates a separate environment to avoid changing any installed software in your system. This is also the easiest way to install the required software especially for the GPU setup.</dl>

        <h3 id="opt">Optimizers</h3>
        <dl>optimizer is an algorithm used to adjust the parameters of a model in order to minimize the 
            <a href="#lossf">error or loss function</a></dl>
        <table>
            <tr>
                <th>Optimizer</th>
                <th>Meaning</th>
                <th>Example</th>
            </tr>
            <tr>
                <td>1. RMSprop (Root Mean Square Propagation)</td>
                <td> It divides the learning rate for a weight by a running average of the magnitudes</td>
                <td>
                    <pre><code>
from keras.optimizers import RMSprop
optimizer = RMSprop(learning_rate=0.001, rho=0.9)
                    </code></pre>
                </td>
            </tr>
            <tr>
                <td>2. Stochastic Gradient Descent (SGD)</td>
                <td>Adjusts the model parameters based on the average gradient of the loss</td>
                <td><pre><code>
from keras.optimizers import SGD
optimizer = SGD(learning_rate=0.01, momentum=0.9)
                </code></pre></td>
            </tr>
        </table>

        <h3 id="overfitting">Overfitting</h3>
        <dl>Means that the model performs well on the training data, but it does not generalize well(ie produces good results on real world/unseen data), because there is too of much uneccessary data(noise) in traning data.</dl>
        <dl><strong>Regularization:</strong> Constraining a model to make it simpler and reduce the risk of overfitting.</dl>

        <h3 id="bias">Bias/Sampling Bias</h3>
        <dl>We should use a training data set that is representative of the cases we want model to predict.</dl>
        <dl>if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.</dl>

        <h3 id="cntk">CNTK</h3>
        <dl>This is Microsoft Cognitive Toolkit (CNTK) backend, plugged with keras.</dl>

        <h3 id="keras">Keras</h3>
        <dl>Library(in Python) which provides functions/APIs to build deep-learning models. Different backends can be plugged with keras</dl>
        <code><pre>
            Keras
            Tensorflow / Theano / CNTK
             CUDA           BLAS,Eigen
             GPU            CPU
        </pre></code>

        <h3 id="layer">Layer / class or Function</h3>
        <dl>Layer processes input data(tensor) and produces an output(tensor) in specific format. 
            Neural network is created by cascading multiple layers.<br>
            Types of Layers:
        </dl>
        <table>
            <tr>
                <th></th>
                <th>Dense Layer / Fully connected layer</th>
                <th>Convolutional Layer</th>
                <th>Recurrent Layer</th>
            </tr>
            <tr>
                <td>What</td>
                <td>Each neuron is connected to every neuron in the previous layer.</td>
                <td>Use convolutional operations to detect local patterns in the input data.</td>
                <td>Processes sequential data, where the order of the input matters</td>
            </tr>
            <tr>
                <td>Usage</td>
                <td>image classification, regression, and more</td>
                <td>image classification, object detection, image segmentation, spatial hierarchies</td>
                <td>natural language processing (NLP), time series analysis, and speech recognition</td>
            </tr>
        </table>

        <h3 id="learningtypes">Learning Types</h3>
        <h4>A. Supervised Learning</h4>
        <dl>The training data feed to the algorithm includes the desired solutions(called labels).</dl>
        <dl>Supervised learning algorithms</dl>
            <dd>k-Nearest Neighbors, <br>
                Linear Regression, <br>Logistic Regression, <br>
                Support Vector Machines (SVMs), <br>Decision Trees & Random Forests, <br>
                Neural networks</dd>
        <dl><strong>Types of Supervised learning</strong></dl>
        <dt><u>1.Classification (give yes/no)</u></dt>
        <dd>a. Spam Filterning: Algo is trained with many example emails along with their class (spam or ham), 
            and it must learn how to classify new emails. Each email has a label.</dd>

        <dt><u>2. Regression: (give % or numeric value)</u></dt>
        <dd>Predictors(Predict something):<br>
            Predict whether based on inputs, Predict price of car provided with some inputs((mileage, age, brand, etc.)</dd>
        
        <dt>3. Logistic Regression (yes/no with %)</dt>
        <dd>Mix of classfication & Regression. Example: 20% of chances being a spam.</dd>

        <h4>B. Unsupervised Learning</h4>
        <dl>Dataset does not have labels. ML model tries to learn without teacher.</dl>
        <dl><strong>Types of unsupervised learning</strong></dl>
        <dt><u>1. Clustering</u></dt>
        <dd>Algorithms used: (k-Means, Hierarchical Cluster Analysis (HCA), Expectation Maximization)</dd>
        <dt><u>2. Visualization and dimensionality reduction</u></dt>
        <dd>
            With unlabelled data this algorithm provides ouput which can be plotted on 2-D, 3-D plane.<br>
            Algorithms used: <br>
            Principal Component Analysis (PCA), Kernel PCA, Locally-Linear Embedding (LLE), t-distributed Stochastic Neighbor Embedding (t-SNE)
        </dd>
        <dt><u>3. Association rule learning</u></dt>
        <dd>
            This provides output as relations between attributes.<br>
            Algorithms used: Apriori, Eclat<br>
            Examples:<br>
            1. Supermarket data analysis: suppose you own a supermarket. Sales logs 
            may reveal that people who purchase sauce, potato chips also buy bread. 
            Thus, you may want to place these items close to each other
        </dd>

        <h4>C. Semisupervised Learning</h4>
        <dt>lot of unlabeled data and a little bit of labeled data<br>
            Algorithms: Deep belief networks (DBNs),<br>
            Examples: 1. FB Photos: When we load photos, we provide labels to few and leave others.
             AI identifies photos.
        </dt>

        <h4>D. Reinforcement learning</h4>
        <dt>Agent(AI Program) can observe the environment, select and perform actions, and get rewards in return
        </dt>

        <h3 id="lossf">Loss Function</h3>
        <dl>In Keras, a loss function is used during the training of a neural network. 
            It measures the difference between the model's predictions and the actual target values. The goal of training is to minimize this loss</dl>
        <code><pre>
            Loss function = (Actual O/P) - (Expected output)
        </pre></code>
        <table>
            <tr>
                <th>Types</th>
                <th>What</th>
                <th>Example</th>
            </tr>
            <tr>
                <td>1. categorical_crossentropy</td>
                <td>used in multi-class classification problems when the target variable is one-hot encoded</td>
                <td>model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])</td>
            </tr>
            <tr>
                <td>2. Binary Crossentropy (binary_crossentropy)</td>
                <td>Used for binary classification problems, where the target variable is binary (0 or 1)</td>
                <td>model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
            </tr>
            <tr>
                <td>3. Mean Squared Error (mean_squared_error or mse)</td>
                <td>Measures the average squared difference between the true and predicted values</td>
                <td>model.compile(optimizer='adam', loss='mean_squared_error')</td>
            </tr>
        </table>

        <h3 id="neuron">Neuron / Node / Function or class</h3>
        <dl>A neuron is the basic unit within a layer. 
            It takes input, performs a computation, and produces an output.</dl>
        <dl>Each neuron has weight, bias, activation function</dl>
        <dt>weight</dt>
        <dd>Neurons receive input signals, and each input is associated with a weight. 
            These weights represent the strength of the connection between the input and the neuron. 
        </dd>
        <pre><code>
class Neuron:
    def __init__(self, num_inputs):
        self.weights = initialize_weights(num_inputs)
        self.bias = initialize_bias()
        self.activation_function = relu

    def forward(self, input_data):
        # Compute the weighted sum of inputs
        weighted_sum = sum(weight * input_value for weight, input_value in zip(self.weights, input_data)) + self.bias
        
        # Apply the activation function
        output = self.activation_function(weighted_sum)
        
        return output        
        </code></pre>

        <h3 id="neuralnetwork">Neural Network</h3>
        <dl>It try to emulate the human brain, combining computer 
            science and statistics to solve common problems in the field of AI</dl>
        <dl>It contains an input layer, one or more hidden layers, and an output layer.</dl>

        <h3 id="tensor">Tensor = n-D Matrix</h3>
        <dl>This is matrix(as in maths). 
            Multi-dimensional numpy arrays used to store numbers during computation.</dl>

        <h4>Types of Tensors</h4>
        <table>
            <tr>
                <th>Dimension/Rank/Axis/Ndim</th>
                <th>Name</th>
                <th>Representation</th>
                <th>Examples</th>
                <th>Shape(Rows,cols)</th>
                <th>Processed By (Keras)</th>
            </tr>
            <tr>
                <td>0</td>
                <td>Scalar</td>
                <td>[0]</td>
                <td></td>
                <td>(0)</td>
                <td></td>
            </tr>
            <tr>
                <td>1</td>
                <td>vector</td>
                <td>[1,2,3,4]</td>
                <td></td>
                <td>(4)</td>
                <td></td>
            </tr>
            <tr>
                <td>2</td>
                <td>Matrix / 2D Tensor</td>
                <td>{{1,2,3},{4,5,6}}</td>
                <td>samples</td>
                <td>(2,3)</td>
                <td>Dense Layer</td>
            </tr>
            <tr>
                <td>3</td>
                <td>3D Tensor</td>
                <td>{{{1,2,3},{4,5,6}},{{1,2,3},{4,5,6}}}</td>
                <td>Timestamped data</td>
                <td>(2,2,3)</td>
                <td>Recurrent layers(eg: LSTM layer)</td>
            </tr>
            <tr>
                <td>4</td>
                <td>4D Tensor</td>
                <td>3D tensors packed together</td>
                <td></td>
                <td>2D convolution layers (Conv2D)</td>
                <td></td>
            </tr>
        </table>
        <code><pre>
            /////////// 2-D Tensor //////////////
            Shape: (2,3)
            Dimension/Rank/Axis/Ndim: 2
                    | 1,2,3 |
                    | 4,5,6 |
            
            /////////// 3-D Tensor example. Packing 2-D matrices //////////////
            Shape: (3,3,5)
            Dimension/Rank/Axis/Ndim: 3
            
            >> x = np.array([[
                                  [5, 78, 2, 34, 0],
                                  [6, 79, 3, 35, 1],
                                  [7, 80, 4, 36, 2]
                             ],
                             [
                                  [5, 78, 2, 34, 0],
                                  [6, 79, 3, 35, 1],
                                  [7, 80, 4, 36, 2]
                             ],
                             [
                                  [5, 78, 2, 34, 0],
                                  [6, 79, 3, 35, 1],
                                  [7, 80, 4, 36, 2]
                            ]])
        </pre></code>

        <h4>Tensor Terms</h4>
        <dl>Data types(dtype)</dl>
        <dd>Type of the data contained in the tensor; for instance, a tensor’s type could be float32, uint8, float64, and so on.</dd>
        <dd>String tensors don’t exist in Numpy (or in most other libraries), because tensors are preallocated contiguous memory segments, and strings, being variable length.</dd>

        <dl>Rank/Axis/Dimension/ndim</dl>
        <dd>Dimension of matrix. For instance, a 3D tensor has three axes, and a matrix has two axes. This is also called the tensor’s ndim in Python libraries such as Numpy.</dd>
        
        <dl>Shape</dl>
        <dd>Tells how many size tensor has along each axis.</dd>
        <dd>For instance, the previous matrix example has shape (3, 5), and the 3D tensor example has shape (3, 3, 5).</dd>

        <h3 id="tensorflow">Tensorflow</h3>
        <dl>This is ML Open source library(EXPOSING APIs) for numerical computation and large-scale ML supports CPUs & GPUs.</dl>
        <dl>Python Front-end APIs & backend written in c++ for high performance.</dl>
        <code><pre>
            //Install conda https://docs.conda.io/projects/miniconda/en/latest/
            C:\Users\amitk\source\repos\Python> mkdir venv_ml1
            C:\Users\amitk\source\repos\Python> cd venv_ml1
            C:\Users\amitk\source\repos\Python\venv_ml1>"c:\Users\amitk\miniconda3\Scripts\activate" venv_ml1
            //Env is created here: C:\Users\amitk\miniconda3\envs
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>"c:\Users\amitk\miniconda3\condabin\deactivate.bat"  //deactivate
            (venv_ml1) C:\Users\amitk\source\repos\Python\venv_ml1>pip install tensorflow
            Downloading tensorflow-2.6.2-cp36-cp36m-win_amd64.whl (423.3 MB)
        </pre></code>

        <h3 id="underfitting">Underfitting</h3>
        <dl>Does not produces good results on traning data.</dl>

        <h3 id="variance">Variance</h3>
        <dl>Variance is the tendency to learn random things unrelated to the real signal</dl>

    </div>
</body>
</html>
